{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSzOuurTMFcmvP4eWk1t0K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**STEMMING**"],"metadata":{"id":"ya0o4w5kfARX"}},{"cell_type":"markdown","source":["The stem of the verb wait is wait: it is the part that is common to all its inflected variants.\n","\n","wait (infinitive)\n","\n","wait (imperative)\n","\n","waits (present, 3rd person, singular)\n","\n","wait (present, other persons and/or plural)\n","\n","waited (simple past)\n","\n","waited (past participle)\n","\n","waiting (progressive)\n"],"metadata":{"id":"YCm0pfdTfH8V"}},{"cell_type":"markdown","source":["**Inflection** is a process of word formation, in which a word is modified to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, mood, animacy, and definiteness"],"metadata":{"id":"ELzGFCeufLoT"}},{"cell_type":"markdown","source":["Sometime spelling may also change in order to make a new word.\n","\n","beauty, duty + -ful → beautiful, dutiful (-y changes to i)\n","\n","heavy, ready + -ness → heaviness, readiness (-y changes to i)\n","\n","able, possible + -ity → ability, possibility (-le changes to il)\n","\n","permit, omit + -ion → permission, omission (-t changes to ss)"],"metadata":{"id":"GFqSAvkrfTX0"}},{"cell_type":"markdown","source":["OverStemming\n","\n","Over-stemming is when two words with different stems are stemmed to the same root. This is also known as a false positive.\n","\n","universal\n","\n","university\n","\n","universe\n","\n","All the above 3 words are stemmed to univers which is wrong behavior."],"metadata":{"id":"lYJW1D-ffb7h"}},{"cell_type":"markdown","source":["**UnderStemming**\n","\n","Under-stemming is when two words that should be stemmed to the same root are not. This is also known as a false negative. Below is the example for the same.\n","\n","alumnus\n","\n","alumni\n","\n","alumnae\n"],"metadata":{"id":"a7xb2g8Qfe6l"}},{"cell_type":"markdown","source":["Truncating Stemming Algorithm"],"metadata":{"id":"PLXR00h4frPD"}},{"cell_type":"markdown","source":["*1. Porter Stemmer*\n","\n","This is one of the most common and gentle stemmer, Its fast but not very precise."],"metadata":{"id":"CUT7-oYYfuKr"}},{"cell_type":"code","source":["pip install nltk"],"metadata":{"id":"1vBXaf2bhGJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"l7akYmT5e-xv","executionInfo":{"status":"ok","timestamp":1684088852760,"user_tz":-60,"elapsed":2833,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}}},"outputs":[],"source":["import nltk\n","from nltk.stem.porter import *"]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiqdw3dxgOZj","executionInfo":{"status":"ok","timestamp":1684088877187,"user_tz":-60,"elapsed":932,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}},"outputId":"bf82a39e-f89c-4616-e258-7093a6084a26"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["porterStemmer = PorterStemmer()"],"metadata":{"id":"LXeAaReAf-VU","executionInfo":{"status":"ok","timestamp":1684088975448,"user_tz":-60,"elapsed":250,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["sentence = \"Provision Maximum multiply owed caring on go gone going was this\""],"metadata":{"id":"otIjh8WjgHhK","executionInfo":{"status":"ok","timestamp":1684088980445,"user_tz":-60,"elapsed":212,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["wordList = nltk.word_tokenize(sentence)\n","\n","stemWords = [porterStemmer.stem(word) for word in wordList]\n","\n","print(' '.join(stemWords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPZy_wRzgI84","executionInfo":{"status":"ok","timestamp":1684089047977,"user_tz":-60,"elapsed":211,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}},"outputId":"7b4b1270-e675-4ef8-8781-d9b7f1dd47ac"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["provis maximum multipli owe care on go gone go wa thi\n"]}]},{"cell_type":"markdown","source":["**Problem?**\n","\n","Look at the input and you can see we are passing “was” and getting “wa” as output. This is something which should be considered under less precise algorithm. To increase the precision another algorithm came which was SnowBall Stemmer."],"metadata":{"id":"1Jk_NjQugVcp"}},{"cell_type":"markdown","source":["*2. Snowball Stemmer*"],"metadata":{"id":"p0C4UCHMgXoC"}},{"cell_type":"markdown","source":["The actual name of this stemmer is English Stemmer or Porter2 Stemmer\n","There were some improvements done on Porter Stemmer which made it more precise over large data-sets"],"metadata":{"id":"_KjuUr2SgcAz"}},{"cell_type":"code","source":["from nltk.stem.snowball import SnowballStemmer\n","\n","snowBallStemmer = SnowballStemmer(\"english\")\n","\n","sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n","wordList = nltk.word_tokenize(sentence)\n","\n","stemWords = [snowBallStemmer.stem(word) for word in wordList]\n","\n","print(' '.join(stemWords))"],"metadata":{"id":"KjilL75ugh6V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As this was an improvement over Porter Stemmer hence we can see in the results how gracefully it handled “was” input. There was lots of improvement done in this algorithm. \n","\n","Including Stopword stemming"],"metadata":{"id":"syFuTDTDu04d"}},{"cell_type":"code","source":["nltk.download('stopwords')\n","from nltk.stem.snowball import SnowballStemmer\n","\n","stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n","\n","sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n","wordList = nltk.word_tokenize(sentence)\n","\n","stemWords = [snowBallStemmer.stem(word) for word in wordList]\n","\n","print(' '.join(stemWords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8anOy1VgvJX0","executionInfo":{"status":"ok","timestamp":1684089379409,"user_tz":-60,"elapsed":214,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}},"outputId":"36b39f6d-b4c4-43e5-a61d-0e735d1c0b07"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["provis maximum multipli owe care on go gone go was this\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["Language support"],"metadata":{"id":"xiadzghzvS-5"}},{"cell_type":"code","source":["print(\" \".join(SnowballStemmer.languages))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFvwkTO3vVqT","executionInfo":{"status":"ok","timestamp":1684089439902,"user_tz":-60,"elapsed":514,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}},"outputId":"f313597e-8ba6-4db2-e148-3ea0e0c24248"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"]}]},{"cell_type":"markdown","source":["3. Lancaster Stemmmer\n","\n","It is very aggressive algorithm\n","\n","It will hugely trim down your working set, this statement itself has pros and cons, sometime you many want this in your datasets but maximum time you will be avoiding it."],"metadata":{"id":"YnP_TKuZvfUN"}},{"cell_type":"code","source":["from nltk.stem.lancaster import *\n","\n","lancasterStemmer = LancasterStemmer()\n","\n","sentence = \"Provision Maximum multiply owed caring on go gone going was this\"\n","wordList = nltk.word_tokenize(sentence)\n","\n","stemWords = [lancasterStemmer.stem(word) for word in wordList]\n","\n","print(' '.join(stemWords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n0b1tNQkvjfS","executionInfo":{"status":"ok","timestamp":1684089519147,"user_tz":-60,"elapsed":248,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}},"outputId":"5a5275e7-b70d-4cf6-c328-a3076b7af56d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["provid maxim multiply ow car on go gon going was thi\n"]}]},{"cell_type":"markdown","source":["Aggression can be observed by “Caring” input, It was converted to “car” which is altogether a different word in English dictionary."],"metadata":{"id":"fBE-plu9vngb"}},{"cell_type":"markdown","source":["Small comparison"],"metadata":{"id":"GdqNQQ5tv1It"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","snowball = SnowballStemmer(language='english')\n","regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n","word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n","print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n","for word in word_list:\n","    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDxLohuMv2Vm","executionInfo":{"status":"ok","timestamp":1684089619884,"user_tz":-60,"elapsed":757,"user":{"displayName":"Arun Kumar Rajasekaran","userId":"08456594014882123910"}},"outputId":"f451ecc2-3f03-4edc-fd57-5006a2d57421"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          \n","friend              friend              friend              friend                        friend                                  \n","friendship          friendship          friendship          friend                        friendship                              \n","friends             friend              friend              friend                        friend                                  \n","friendships         friendship          friendship          friend                        friendship                              \n"]}]}]}